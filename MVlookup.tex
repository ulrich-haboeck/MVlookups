%\documentclass[10pt,a4paper,oneside]{article}
% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[11pt]{article}

%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[total={7in,9in}]{geometry}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{lmodern}
\usepackage[bookmarks, colorlinks=false, pdftitle={Lookup Arguments based on Logarithmic Derivatives}, pdfauthor={Ulrich Haboeck}]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{tikz}
%\usepackage{titlesec}
\usepackage{float}
\usetikzlibrary{shapes, fit}
\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{2}
\setlength{\marginparwidth}{3cm}



\usepackage{url}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{nicefrac}

\usepackage[n,advantage,operators,sets,adversary,landau,probability,notions,logic,ff,mm,primitives,events, complexity,asymptotics,keys]{cryptocode}

\usepackage{listings}
\usepackage{footnote}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{%frame=tb,https://www.overleaf.com/project/608bc77c801b16bbadb2210a
  language=sh,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\RequirePackage{etex}

% Theorem environments %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{thm}{Theorem}[]
\newtheorem*{thm*}{Theorem}
\newtheorem{cor}{Corollary}[]
\newtheorem{lem}[]{Lemma}
\newtheorem{prop}[]{Proposition}
\newtheorem{conj}[]{Conjecture}
\newtheorem{protocol}[]{Protocol}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{defn*}{Definition}

\theoremstyle{definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{rem*}[]{Remark}

% MATH %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\DeclareMathOperator{\N}{\mathbb{N}}
\renewcommand{\PP}{\mathbf{P}}
\newcommand{\OO}{\mathcal{O}}


\DeclareMathOperator{\param}{\mathsf{Par}}
\DeclareMathOperator{\gen}{\mathsf{Gen}}
\DeclareMathOperator{\setup}{\mathsf{Setup}}
\DeclareMathOperator{\indexer}{\mathsf{Index}}
\DeclareMathOperator{\comm}{\mathsf{Com}}
\DeclareMathOperator{\open}{\mathsf{Open}}
\DeclareMathOperator{\prove}{\mathsf{Prove}}
\DeclareMathOperator{\extract}{\mathsf{Extract}}
\DeclareMathOperator{\simulate}{\mathsf{Sim}}
\DeclareMathOperator{\RS}{\mathsf{RS}}
\DeclareMathOperator{\FFT}{\mathsf{FFT}}
\DeclareMathOperator{\Quotient}{\mathsf{Quotient}}
\DeclareMathOperator{\agree}{\mathsf{agree}}

\renewcommand{\adv}{\mathsf{Adv}}


\author{%
Ulrich Hab{\"o}ck
\\\\
Orbis Labs, Polygon Zero
\\
\texttt{uhaboeck@polygon.technology}
}

%\date{%
%\today
%%\footnote{%
%%
%%}
%} 



\begin{document}

%\frontmatter
\title{%
Multivariate lookups based on
logarithmic derivatives
}
\date{%
\today\footnote{%
This updated version clarifies the trade-off between algebraic degree and the number of commitments, previously expressed by two protocol variants (one for small and one for large numbers of columns).
It further discusses Polygon Miden's bounded multiplicity encoding, applied to domain-sized table lookups.
}
}
\maketitle

\begin{abstract}
Logarithmic derivatives translate products of linear factors into sums of their reciprocals, turning zeroes into simple poles of same multiplicity.
Based on this simple fact, we construct an interactive oracle proof for lookups over the boolean hypercube, which makes use of a single multiplicity function instead of working with a rearranged union of table and witnesses, as Plookup \cite{Plookup} does.
For single-column lookups the performance is comparable to Plookup, taken to the multivariate setting by Hyperplonk+ \cite{Hyperplonk}.
However, the real power of our argument unfolds in the case of ``batch-column'' lookups, where multiple columns are subject to the same table lookup:  
While the number of field operations is comparable to the Plookup strategy, the oracles provided by our prover are significantly fewer. 
For example, given $M=20$ columns of length between $2^{12}$ and $2^{18}$ , and assuming a commitment scheme over an elliptic curve with a $256$ bit large base field, our paper-pencil operation counts indicate that the logarithmic derivative lookup is more than $3.9$ times faster.
\end{abstract}
%Keywords: SNARKs, recursive proofs, aggregation scheme

%\begin{KeepFromToc}

\tableofcontents
%\end{KeepFromToc}

%\mainmatter
\section{Introduction}

Lookup arguments prove a sequence of values being member of an, often prediscribed, table.
They are an essential tool for improving the efficiency of arguments for statements which are otherwise quite expensive to arithmetize. 
Main applications are (1) lookups for relations of high algebraic complexity, and (2) lookups for interval ranges, the latter of which are extensively used by zero-knowledge virtual machines.
% enforcing execution trace elements being valid machine words.
Although closely related to permutation (or shuffle) arguments \cite{shuffle}, a first explicit occurence of lookups dates back to Arya \cite{Arya}, to the best of our knowledge.
While their argument handles multiplicities directly in a quite costly manner, Plookup \cite{Plookup} greatly improved over \cite{Arya} using a rather geometric approach.
Since then Plookup (and variants of it) is \textit{the} general purpose lookup argument used in practical applications. 
%That strategy, which we call the Plookup strategy in the sequel, uses a rearranged concatenation of witness and table sequence

In this paper we describe a lookup argument based which is based on logarithmic derivatives. 
As in classical calculus, formal logarithmic derivatives turn products $\prod_{i=1}^N (X - z_i)$ into sums of their reciprocals, 
\[
\sum_{i=1}^N \frac{1}{X - z_i},
\]
having poles with the same multiplicity as the zeros of the product.
Working with poles instead of zeros is extremly useful for lookup arguments.
%This fractional decomposition of the logarithmic derivative is extremly useful for lookup arguments:
While the treatment of multiplicities is costly in the product approach, it turns cheap when using logarithmic derivatives:
Given a sequence of field elements $(a_i)_{i=1}^N$ and another sequence $(t_j)_{j=1}^M$, then $\{a_i: i =1,\ldots, N\}\subseteq \{t_j : j=1,\ldots, M\}$ as sets, if and only if there exists a sequence of field elements $(m_j)_{j=1}^M$ (the multiplicities) such that
\begin{equation*}
\label{e:intro:lookup:eq}
\sum_{i=1}^N \frac{1}{X - a_i}  = \sum_{j=1}^M \frac{m_j}{X - t_j}.
\end{equation*}
 (This holds under quite mild conditions on the field, see Lemma \ref{lem:batchsetmembership} for details.)
%In particular for batch-column lookups, where several columns are subject to the same table, 
Based on this fractional identity we construct a lookup proof which is more efficient than the Plookup approach, which argues via a sorted union of witness and table sequence.
This is particularly true in the case of \textit{batch-column} lookups, where several sequences (``columns'') are subject to the same table lookup, a situation that often arises in zero-knowledge virtual machines,  enforcing execution trace elements being valid machine words.
For example, the arithmetic unit of Polygon's System Zero \cite{PolygonZero} has about $70$ columns subject to one and the same range check, or the tinyRAM implementation of Orbis Labs \cite{tinyRAMOrbis} has $12$ columns subject to one and the same lookup table. 
%The naive strategy for processing batches of columns is to run several lookup proofs in parallel, one for each column. 
%However this leads to a vast computational overhead.
%Independent on the number of columns to be looked up, one still works with a single multiplicity function. 
In our lookup the oracle costs, measuring the number and sizes of the oracles, are significantly lower than for a lookup based on the Plookup strategy.

%Although our implementation is not ready yet, we compare the two strategies by detailed operation counts, using a benchmark-backed measure for the cost of multi-scalar multiplications in terms of field multiplications.  
%For columns of length $2^{12}$, and considering an elliptic curve over a 256 bit large prime field, our counts indicate a speedup by a factor between $1.5$ and $2.5$, depending on $M$ the number of columns. 
%(For a large numbers of plumns this factor is independent of $M$ and about $1.8$.) 
%Although we describe our lookups in the multivariate setting, we point out  

We stress the fact that we are not the only ones who exploit fractional decompositions for lookups. 
Concurrently, building on the work of \cite{Caulk} and \cite{CaulkPlus} on ``large-table'' lookups, Gabizon and Khovratovich \cite{flookup} describe a bilinear argument, the proving-time of which is independent of the table size. 
The main contribution of \cite{flookup} is a univariate oracle proof for the radical of a witness sequence (i.e. the sequence with multiple occurences removed) which is almost identical to our approach\footnote{%
Almost simultaneously, G. Roh, W. Dai, M. Jabbour, and A. He published the same idea in a blog post \cite{DaiFlookupBlog}.
To underpin that our work was not influenced by concurrent efforts we refer to the verified commit on our Github repository, 
\url{https://github.com/Orbis-Tertius/MVlookups/commit/4a8816a04e8107e05d4bb0897ee52e72210c84b8} 
}.
%Instead of using logarithmic derivatives, their prover explicitly provides the polynomial $R_T(X) = \sum_{j=1}^M m_j \cdot \frac{v_T(X)}{X - t_j}$, where $v_T(X)= \prod_{j=1}^M (X - t_j)$ is the precomputed table polynomial, and shows the identity 
%\[
%\sum_{i=1}^N \frac{v_T(X)}{X - a_i}  = R_T(X).
%\]
%While the oracle costs, measured by the number and sizes of the polynomials, are less than in our argument, that advantage is traded for the computation of $R_T(X)$ in the ring of polynomials, which in general consumes $O(M\cdot\log^2 M)$ field operations.
%We give comparison with our approach in the appendix. 

This document focuses on batch-column lookups with asymptotically linear prover costs.  
It is organized as follows.
%In particular, we consider batch-column lookups with respect to a single, in practice medium-sized table, a use case that is extensively used in execution trace proofs.
%The document s organized as follows. 
In Section \ref{s:preliminaries}, we gather the preliminaries used in the sequel: 
The Lagrange kernel over the boolean hypercube,  basic facts on the formal logarithmic derivative, and a summary of the multivariate sumcheck argument. 
Besides that, we informally introduce Lagrange interactive oracle proofs, an oracle model we consider suitable for arguments which are based on the Lagrange representation of polynomials rather than their coefficients.   
In Section \ref{s:lookups} we describe our lookup argument based on the logarithmic derivative. 
For comparison reasons, we add an extra section (Section \ref{s:hyperplonk}) in which we outline batch-column lookups using the Plookup strategy, adapted to the boolean hypercube. 
These rely on the time shift from Hyperplonk \cite{Hyperplonk}, and we consider them state-of-the-art in the multivariate setting.
%Eventually, in Appendix \ref{s:appendix} we recap the univariate Plookup argument, sketch inner product arguments for Lagrange queries,  and show how to turn the multivariate KZG \cite{Kate} commitment scheme into a scheme for Lagrange queries, which does not access the coefficients at any point in time.

We finally point out, that although our protocol is written for the multilinear setting, its translation into a univariate proof is straight-forward.
We expect these univariate arguments to improve similarly over multi-column lookups based on the Plookup strategy.






\section{Preliminaries}
\label{s:preliminaries}

\subsection{The Lagrange kernel of the boolean hypercube}

Let $F$ denote a finite field, and $F^*$ its multiplicative group.
Throughout the document we regard the boolean hypercube $H= \{\pm 1\}^n$  as a multiplicative subgroup of  $(F^*)^n$.
For a multivariate function $f(X_1,\ldots, X_n)$, we will often use the vector notation $\vec X = (X_1,\ldots, X_n)$ for its arguments, writing $f(\vec X) := f(X_1,\ldots, X_n)$.
 

%Given a function $f: H\rightarrow F$, its \textit{Langrange interpolation} is the unique multilinear polynomial $p(\vec X)$ in $\vec X = (X_1,\ldots, X_n)$ such that $p(\vec x) = f(\vec x)$ for every $\vec x\in H$.
%As $H$ has an increasing sequence of subgroups $\{1\} = H_0\subset H_1 \subset \ldots \subset H_n = H$, each $H_i$ having order $|H_i| = 2^i$,  Lagrange interpolation can be done in only $2^n$ field multiplications\footnotemark and $n\cdot 2^n$ additions and substractions, compared to $n\cdot 2^{n-1}$ multiplications and additions as for univariate interpolation from order $2^n$ subgroups of $F^*$.
%\footnotetext{%
%The field multiplications are due to the normalization of $f$ by the factor $\frac{1}{2^n}$, and can be entirely omitted an IOP. 
%One ``commits'' to the non-normalized Lagrange interpolant and corrects the queried evaluations.
%}%
%See Appendix \ref{s:appendix} for details. 
The \textit{Lagrange kernel} of $H$ is the multilinear polynomial
\begin{equation}
\label{e:LagrangeKernel}
L_H(\vec X, \vec Y)  = \frac{1}{2^n}\cdot \prod_{j=1}^n (1 + X_j\cdot Y_j).
\end{equation}
Notice that $L_H(\vec X, \vec Y)$ is symmetric in $\vec X$ and $\vec Y$, i.e. $L_H(\vec X, \vec Y)=L_H(\vec Y, \vec X)$, and that \eqref{e:LagrangeKernel} is evaluated within only $\bigO{\log|H|}$ field operations.
Whenever $\vec y \in H$ we have that $L_H(\vec X, \vec y)$ is the Lagrange polynomial on $H$, which is the unique multilinear polynomial which satisfies $L_H(\vec x, \vec y) = 1$ at $\vec x = \vec y$, and zero elsewhere on $H$.
In particular for a function $f: H\rightarrow F$  the inner product evaluation formula 
\[
\left\langle f ,L_H(\:.\:, \vec y)\right\rangle_H := \sum_{\vec x\in H} f(\vec x) \cdot L_H(\vec x, \vec y) = f(\vec y).
\]
is valid for every $\vec y\in H$.
This property extends beyond $H$, as the following Lemma shows.
\begin{lem}
\label{lem:Lagrange}
Let $p(\vec X)$ be the unique multilinear extension of $f: H\rightarrow F$. 
Then for every $\vec y\in F^n$,
\begin{equation}
\label{e:LagrangeScalarProduct}
\left\langle f ,L_H(\:.\:, \vec y)\right\rangle_H = \sum_{x\in H} f(\vec x) \cdot L_H(\vec x, \vec y) = p(\vec y).
\end{equation}
\end{lem}
\begin{proof}
%This is straight forward from the Lagrange representation $p(X)=\sum_{\vec z\in H} f(\vec z) \cdot L_H(\:.\:, \vec z)$.
Since $p(\vec y) = \sum_{\vec x\in H} f(\vec z)\cdot L_H(\vec X,\vec z)$, it suffices to show the claim for $p(X) = L_H(\vec X,\vec z)$, with $\vec z\in H$.
By the property of $L_H(\vec X,\vec z)$, we have $\big\langle L_H(\:.\:, \vec z), L_H(\:.\:,\vec y) \big\rangle_H =L_H(\vec y,\vec z)$, which by symmetry is equal to $L_H(\vec X,\vec y)$ at $\vec X=\vec z$.
This completes the proof of the Lemma.
\end{proof}


%This is tantamount to the tensor-query to point-query paradigm from \cite{TensorIOP} used in a line of work, e.g. \cite{TensorCodes, TensorR1CS, TensorRothblum, TensorR1CSarbitraryF}.
Note that for any $\vec y\in F^n$, the domain evaluation of $L_H(\vec X, \vec y)$ over $H$ can be computed in 
$\bigO{|H|}$ field operations, by recursively computing the domain evaluation of the partial products 
$
p_k(X_1,\ldots, X_k, y_1,\ldots, y_k)= \frac{1}{2^n}\cdot \prod_{j=1}^k (1 + X_j\cdot y_j)
$ 
over $H_k =\{\pm 1\}^k$ from the domain evaluation of $p_{k-1}$, where one starts with $f_0 = \frac{1}{2^n}$ over the single-point domain $H_0$.
Each recursion step costs $|H_{k-1}|$ field multiplications, denoted by $\mathsf M$, and the same number of additions, denoted by $\mathsf A$,  yielding overall
\begin{equation}
\label{e:lagrange:cost}
\sum_{k=1}^{n} |H_{k-1}| \cdot (\textsf M + \textsf A) < |H| \cdot  (\textsf M + \textsf A).
\end{equation}


\subsection{The formal derivate}

Given a univariate polynomial $p(X) =\sum_{k=0}^{d} c_k\cdot X^k$ over a general (possibly infinite) field $F$, its \textit{derivative} is defined as 
\begin{equation}
\label{e:DerivativePoly}
p'(X) := \sum_{k=1}^{d} k \cdot c_k \cdot X^{k-1}.
\end{equation}
As in calculus, the derivative is linear, i.e. for every two polynomials $p_1(X), p_1(X)\in F[X]$, and coefficients $\lambda_1,\lambda_2\in F$,
\begin{equation*}
%\label{e:DerivativeLinear}
(\lambda_1 \cdot p_1(X) + \lambda_2 \cdot p_1(X))' = \lambda_1\cdot p_1'(X) + \lambda_2\cdot p_2'(X)
\end{equation*}
 and we have the product rule
\begin{equation*}
%\label{e:ProductRule}
(p_1(X)\cdot p_2(X))' = p_1'(X)\cdot p_2(X) + p_1(X)\cdot p_2'(X).
\end{equation*}
For a function $\frac{p(X)}{q(X)}$ from the rational function field $F(X)$, the derivative is defined as the rational function
\begin{equation}
\label{e:DerivativeQuotient}
\left(\frac{p(X)}{q(X)}\right)' := \frac{p'(X)\cdot q(X) - p(X)\cdot q'(X)}{q(X)^2}.
\end{equation}
By the product rule for polynomials, the definition does not depend on the representation of $\frac{p(X)}{q(X)}$.
Both linearity as well as the product rule extend to rational functions. 
%For a proof of these facts, as well as alternative definitions for the formal derivative, we refer to standard literature. 

For any polynomial $p(X)\in F[X]$, if $p'(X)=0$ then $p(X)= g(X^p)$ where $p$ is the characteristic of the field $F$.
In particular, if $\deg p(X) < p$, then the polynomial must be constant.
As the analogous fact for fractions is not as commonly known, we give a proof of the following lemma.

\begin{lem}
\label{lem:DerivativeFraction}
Let  $F$ be a field of characteristic $p\neq 0$, and $\frac{p(X)}{q(X)}$ a rational function over $F$ with both  $\deg p(X) < p$ and $\deg q(X) < p$.
If the formal derivative $\left(\frac{p(X)}{q(X)}\right)' = 0$, then $\frac{p(X)}{q(X)} = c$ for some constant $c\in F$.
\end{lem}

\begin{proof}
If $q(X)$ is a constant, then the assertion of the Lemma follows from the corresponding statement for polynomials.
Hence we assume that $\deg q(X)>0$.
Use polynomial division to obtain the representation
\[
\frac{p(X)}{q(X)} = m(X) + \frac{r(X)}{q(X)},
\]
with $m(X), r(X) \in F[X]$, $\deg m(X) \leq \deg p(X)$, and $\deg r(X) < \deg q(X)$ whenever $r(X)\neq 0$.
By linearity of the derivative, we have
$
0 =  \left(\frac{p(X)}{q(X)}\right)' = m'(X) + \left(\frac{r(X)}{q(X)}\right)',
$
and therefore
%\[
%\frac{r'(X)\cdot q(X) - r(X)\cdot q'(X)}{q(X)^2} = - m'(X)
%\]
\begin{equation}
\label{e:der}
r'(X)\cdot q(X) - r(X)\cdot q'(X) = - m'(X)\cdot q(X)^2.
\end{equation}
Comparing the degrees of left and right hand side in \eqref{e:der}, we conclude that  $m'(X) = 0$.
Since $\deg m(X) \leq  \deg p(X) < p$ we have $m(X)= c$ for some constant\footnotemark $c\in F$. 
\footnotetext{%
For general degrees of $p(X)$ we would only be able to conclude that $m(X) = g(X^p)$ for some polynomial $g(X)$. 
}% 
Furthermore, if we had $r(X)\neq 0$ then the leading term of the left hand side in \eqref{e:der} would be
\[
%m\cdot d_m \cdot X^{m-1}\cdot c_n \cdot X^n + d_m \cdot X^{m-1}\cdot n\cdot  c_n \cdot X^{n-1} = 
(k - n) \cdot c_n\cdot d_{k} \cdot X^{n + k - 1},
\]
with $c_n \cdot X^n$, $n>0$, being the leading term of $q(X)$, and  $d_k \cdot X^k$, $0\leq k < n$, the leading term of $r(X)$.
As  $0 < n - k < p$, and both $c_n\neq 0$ and $c_m\neq 0$, the leading term of the left hand side of \eqref{e:der} would not vanish.
Therefore it must hold that  $r(X) = 0$ and the proof of the lemma is complete.
\end{proof}


\subsection{The  logarithmic derivative}

The \textit{logarithmic derivate} of a polynomial $p(X)$ over a (general) field $F$ is the rational function
\begin{equation*}
\frac{p'(X)}{p(X)}.
\end{equation*}
Note that the logarithmic derivative of the product $p_1(X)\cdot p_2(X)$ of two polynomials $p_1(X), p_2(X)$ equals the sum of their logarithmic derivatives, since by the product rule we have 
\[
\frac{(p_1(X)\cdot p_2(X))'}{p_1(X)\cdot p_2(X)} = \frac{p_1'(X)\cdot p_2(X) + p_1(X)\cdot p_2'(X)}{p_1(X)\cdot p_2(X)} 
= \frac{p_1'(X)}{p_1(X)} + \frac{p_2'(X)}{p_2(X)}.
\]
In particular the logarithmic derivative of a product $p(X) = \prod_{i=1}^n (X + z_i)$, with each $z_i\in F$, is equal to the sum
\begin{equation}
\label{e:LogDerivativeProduct}
\frac{p'(X)}{p(X)} %= \sum_{i=1}^n \frac{\prod_{j\in \{1,\ldots, n\}\setminus \{i\}} (X - z_j) }{p(X)} 
= \sum_{i=1}^n \frac{1}{X + z_i}.
\end{equation}

The following lemma\footnotemark is a simple consequence of Lemma \ref{lem:DerivativeFraction} and essentially states that, under quite mild conditions on the field $F$, if two normalized polynomials have the same logarithmic derivative then they are equal. 
We state this fact for our use case of product representations.
\footnotetext{%
At the time of writing we learned that also others are aware of Lemma \ref{lem:LogarithmicDerivative}. 
In a Delendum Frontiers session on multi-party computation (6. Oct. 2022) an audience member pointed out its usefulness for permutation arguments.
}%
\begin{lem}
\label{lem:LogarithmicDerivative}
Let $(a_i)_{i=1}^n$ and $(b_i)_{i=1}^n$ be sequences  over a field $F$ with characteristic $p > n$. 
Then
$
\prod_{i=1}^n \left(X + a_i \right) =\prod_{i=1}^n \left(X + b_i \right)
$
in $F[X]$ if and only if  
\begin{equation*}
%\label{e:LogDerivativeSum}
\sum_{i=1}^n \frac{1}{X + a_i} =\sum_{i=1} ^n\frac{1}{X + b_i}
\end{equation*}
in the rational function field $F(X)$.
\end{lem}

\begin{proof}
If  $p_a(X) = \prod_{i=1}^n \left(X + a_i\right)$ and $p_b(X) = \prod_{i=1}^n \left(X + b_i\right)$
coincide, so do their logarithmic derivatives.
To show the other direction, assume that 
$
\frac{p_a'(X)}{p_a(X)}  = \frac{p_b'(X)}{p_b(X)}.
$
Then 
\[
\left(\frac{p_a(X)}{p_b(X)}\right)'  = \frac{p_a'(X)\cdot p_b(X) - p_a(X)\cdot  p_b'(X)} {p_b^2(X)} = 0.
\]
Hence by Lemma \ref{lem:DerivativeFraction} we have $\frac{p_a(X)}{p_b(X)} = c$ for some constant  $c \in F$.
As both $p_a(X)$ and $p_b(X)$ have leading coefficient equal to $1$, we conclude that $c =1$, and the proof of the Lemma is complete.
\end{proof}
\begin{rem}
\label{rem:LogarithmicDerivativeFunctionField}
We stress the fact that Lemma \ref{lem:LogarithmicDerivative} also applies to the case where $F$  is the function field $F_p(Y_1,\ldots, Y_k)$ over a finite field $F_p$ of characteristic $p$.
This observation will be useful when generalizing the permutation argument to the case where $a_i$ and $b_i$ are multilinear polynomials in $Y_1, \ldots, Y_n$.
\end{rem}

Given a product $p(X)=\prod_{i=1}^N (X + a_i)$ we can gather the poles of its logarithmic derivative obtaining the fractional decomposition  
\begin{align*}
\frac{p'(X)}{p(X)} = \sum_{a\in F} \frac{m(a)}{X + a},
\end{align*}
where $m(a)\in \{1,\ldots, N\}$ is the multiplicity of the value $a$ in $(a_i)_{i=1}^N$.
Fractional decompositions are unique, as shown by the following lemma.
\begin{lem}
\label{lem:UniqueFractionalRep}
Let $F$ be an arbitrary field and $m_1, m_2: F\rightarrow F$ any functions.
Then
$
\sum_{z\in F} \frac{m_1(z)}{X - z} = \sum_{z\in F} \frac{m_2(z)}{X - z}
$
in the rational function field $F(X)$, if and only if $m_1(z)=m_2(z)$ for every $z\in F$.
\end{lem}
\begin{proof}
Suppose that the fractional decompositions are equal.  
Then $\sum_{z\in F} \frac{m_1(z)-m_2(z)}{X - z} = 0$, and therefore
\[
p(X) = \prod_{w\in F} (X - w)\cdot\sum_{z\in F} \frac{m_1(z)-m_2(z)}{X - z} = \sum_{z\in F} (m_1(z)-m_2(z))\cdot \prod_{w\in F\setminus\{z\}} (X - w) = 0.
\]
In particular,
$
p(z) = (m_1(z) - m_2(z)) \cdot  \prod_{w\in F\setminus\{z\}} (z - w)= 0
$
for every $z\in F$.
Since   $\prod_{w\in F\setminus\{z\}} (z - w) \neq 0$ we must have $m_1(z)  = m_2(z)$ for every $z\in F$. 
The other direction is obvious.
\end{proof}

This leads to the following algebraic criterion for set membership, which is the key tool for our lookup arguments.
\begin{lem}[Set inclusion]
\label{lem:batchsetmembership}
Let $F$ be a field of characteristic $p>N$, and suppose that $(a_i)_{i=1}^N$, $(b_i)_{i=1}^N$ are arbitrary sequences of field elements.
Then $\{a_i \}\subseteq \{b_i\}$ as sets (with multiples of values removed), if and only if there exists a sequence $(m_i)_{i=1}^N$ of field elements from $F_q\subseteq F$ such that
\begin{equation}
\label{e:fracs}
\sum_{i=1}^N \frac{1}{X + a_i} = \sum_{i=1}^N \frac{m_i}{X + b_i}  
\end{equation}
in the function field $F(X)$.
Moreover, we have equality of the sets $\{a_i\} = \{b_i\}$, if and only if $m_i\neq 0$, for every $i=1,\ldots, N$.
\end{lem}

\begin{proof}%[Proof of Lemma \ref{lem:batchsetmembership}]
Let us denote by $m_a(z)$ the multiplicity of a field element $z$ in the sequence $(a_i)_{i=1}^N$.
Likewise, we do for $(b_i)_{i=1}^N$.
Note that since $N < p$, the multiplicities can be regarded as non-zero elements from $F_p$ as a subset of $F$.
Suppose that $\{a_i\}\subseteq \{b_i\}$ as sets. 
Set $(m_i)$ as the normalized multiplicities
$
m_i = \frac{m_a(b_i)}{m_b(b_i)}.
$
This choice of $(m_i)$ obviously satisfies \eqref{e:fracs}.

Conversely, suppose that \eqref{e:fracs} holds.
Collecting fractions with the same denominator we obtain fractional representations for both sides of the equation \eqref{e:fracs},  
\begin{align*}
\sum_{i=1}^N \frac{1}{X + a_i} &= \sum_{z\in F} \frac{m_a(z)}{X + z},
\\
\sum_{i=1}^N \frac{m_i}{X + b_i} & = \sum_{z\in F} \frac{\mu (z)}{X + z}.
\end{align*}
Note that since $N < p$, we know that for each $z\in \{a_i\}$ we have $m_a(z)\neq 0$. 
By the uniqueness of fractional representations,  Lemma \ref{lem:UniqueFractionalRep}, $m_a(z) = \mu(z)$ for every $z\in \{a_i\}$, and therefore each $z\in \{a_i\}$ must occur also in $\{b_i\}$. 
\end{proof}



\subsection{Lagrange interactive oracle proofs}

The oracle proofs of many general purpose SNARKs such as Plonk \cite{Plonk} or algebraic intermediate representations \cite{Starks} rely on witnesses that are given in Lagrange representation, i.e. by their values over a domain $H$.
Their multivariate variants may completely avoid the usage of fast Fourier transforms whenever the polynomial commitment scheme can be turned into one that does not need to know the coefficients, neither when computing a commitment nor in an opening proof.
Exactly this property is captured by \textit{Lagrange oracle proofs}, rather than polynomial ones \cite{Dark}.

A \textit{Lagrange interactive oracle proof} (\textit{Lagrange IOP}) over the boolean hypercube $H=\{\pm 1\}^n$ is an interactive protocol between two parties, the ``prover'' and the ``verifier''.
In each round, the verifier sends a message (typically a random challenge) and the prover computes one or several functions over the boolean hypercube, and gives the verifier oracle access to them.
From the moment on it is given access, the verifier is allowed to query the oracles for their inner products with the Lagrange kernel $L_H(\:.\:, \vec y)$, associated with an arbitrary vector $\vec y\in F^n$. 

The security notions for Lagrange IOPs, such as completeness, (knowledge) soundness, and zero-knowledge, are exactly the same as for other interactive oracle proofs.
We assume that the reader is familiear with these, and refer to \cite{IOPs} or \cite{Dark} for their formal definitions.

Lagrange IOPs are turned into arguments by instantiating the Lagrange oracles by a  \textit{Lagrange commitment scheme}.
A Lagrange commitment scheme is a commitment scheme for functions over $H$ that comes with an evaluation proof for Lagrange queries.
For example, inner product arguments \cite{BootleGroth} can be directly used to construct Lagrange commitment schemes,
% (see Appendix \ref{s:appendix: IPA}), 
but also the multilinear variant \cite{MVKZG} of the \cite{Kate} commitment scheme is easily modified to completely avoid dealing with coefficients. 
We suppose that this is well-known, and therefore we omit an explicit elaboration in this paper.


\subsection{The sumcheck protocol}

We give a concise summary on the multivariate sumcheck protocol \cite{sumcheck}.
Given a multivariate polynomial $p(X_1,\ldots, X_n)\in F[X_1,\ldots, X_n]$, a prover wants to convince a verifier upon that
\begin{equation*}
s = \sum_{(x_1,\ldots, x_n) \in \{\pm 1\}^n} p(x_1, \ldots, x_n).
\end{equation*}
This is done by a random folding procedure which, starting with $H_0=\{\pm 1\}^n$, which stepwise reduces a claim on the sum over $H_i = \{\pm 1\}^{n-i}$, $i=0,\ldots, n-1$, to one over the hypercube $H_{i+1}$ of half the size. 
Eventually, one ends up with a claim over a single-point sum, which is paraphrased as the value of $p(X_1,\ldots, X_n)$ at a random point $(r_1,\ldots, r_n)\in F^n$ sampled in the course of the reduction steps.

%The reduction principle is best explained in the case of multilinear polynomials $p(X_1,\ldots, X_n)$.
%In the first round the prover provides the values of the partial sums
%\[
%s_1(x_1) = \sum_{(x_2,\ldots, x_n) \in \{\pm 1\}^{n-1}} p(x_1, x_2, \ldots, x_n),
%\]
%for $x_1\in\{\pm 1\}$, which the verifier checks to sum up to the claimed value, i.e. $v= s_1(-1) + s_1(+1)$. 
%If so,  the verifier samples a random $r_1\sample F$ and both prover and verifier continue on the protocol on the linear combination
%\begin{align*}
%r_1 \cdot s_1(+1) + (1 - r_1) \cdot s_1(-1) &= r_1\cdot\hspace*{-1cm}\sum_{(x_2,\ldots, x_n) \in \{\pm 1\}^{n-1}}  p( +1 , x_2, \ldots, x_n) +  (1-r_1)\cdot\hspace*{-1cm}\sum_{(x_2,\ldots, x_n) \in \{\pm 1\}^{n-1}}  p( -1 , x_2, \ldots, x_n)
%\\
%&= \sum_{(x_2,\ldots, x_n) \in \{\pm 1\}^{n-1}} p(r_1 , x_2, \ldots, x_n),
%\end{align*}
%where the latter equality holds as $p(X_1,\ldots, X_n)$ is a linear polynomial in $X_1$.
%After $n$ reduction steps of this kind, the initial claim is eventually reduced to the evaluation claim for $p(X_1,\ldots, X_n)$ at 
%$(X_1,\ldots, X_n) = (r_1, \ldots, r_n)$.
 
%\begin{definition}[Sumcheck protocol, \cite{sumcheck}]
%Let $p(X_1,\ldots, X_n)$ be a multivariate polynomial over a finite field $F$. %with individual degrees $d_i=\deg_{X_i} p(X_1,\ldots, X_n)$.
%The sumcheck protocol, in which a prover wants to convince the verifier upon the sum $s = \sum_{(x_1,\ldots, x_n) \in \{\pm 1\}^n} p(x_1, \ldots, x_n)$, is as follows.
%\begin{itemize}
%\item 
%In the first round $i=1$, the prover sends (the coefficients of) the univariate polynomial 
%\[
%s_1(X) = \sum_{(x_{2},\ldots, x_n) \in \{\pm 1\}^{n-1}} p(X ,x_{2}, \ldots, x_n)
%\]
%of degree $d_1\leq \deg_{X_1} p(X_1,\ldots, X_n)$, to the verifier.
%%(This polynomial is computed in linear time from its values over a set $D_1\supseteq \{\pm 1\}$ of size $|D_1| = d_1 + 1$.) 
%The verifier checks if 
%\[
%v = s_1(-1) + s_1(+1),
%\] 
%and if so it responds with a random challenge $r_1$ sampled uniformly from $F$.
%
%\item
%In each of the further rounds $i=2,\ldots, n$, the prover sends the univariate polynomial of degree $d_i \leq \deg_{X_i} p(X_1,\ldots, X_n)$ given by
%\[
%s_i(X) = \sum_{(x_{i+1},\ldots, x_n) \in \{\pm 1\}^{n-i}} p(r_1,\ldots, r_{i-1},X ,x_{i+1}, \ldots, x_n),
%\]
%where $r_1, \ldots, r_{i-1}$ are the randomnesses received in the previous rounds.
%%(Again, the computation is done by interpolation from the values over a set $D_i\supseteq \{\pm 1\}$ of size $|D_i| = d_i + 1$.)
%The prover sends the coefficients of $s_{i}(X)$ to the verifier, which checks whether 
%\[
%s_{i-1}(r_{i-1}) = s_{i}(+1) + s_{i}(-1).
%\] 
%If so, the verifier sends another random challenge $r_i\sample F$ to the prover.
%\end{itemize}
%After these rounds the verifier checks if $s_n(r_n) = p(r_1,\ldots, r_n)$. 
%If so, the verifier accepts (otherwise it rejects).  
%\end{definition}

\begin{protocol}[Sumcheck protocol, \cite{sumcheck}]
\label{p:Sumcheck}
Let $p(X_1,\ldots, X_n)$ be a multivariate polynomial over a finite field $F$. %with individual degrees $d_i=\deg_{X_i} p(X_1,\ldots, X_n)$.
The sumcheck protocol, in which a prover wants to convince the verifier upon the sum $s = \sum_{(x_1,\ldots, x_n) \in \{\pm 1\}^n} p(x_1, \ldots, x_n)$, is as follows.
We write $s_0(X)$ for the constant polynomial $s_0 =s$.
\begin{itemize}
\item
In each round $i=1,\ldots, n$, the prover sends the coefficients of the univariate polynomial 
\[
s_i(X) = \sum_{(x_{i+1},\ldots, x_n) \in \{\pm 1\}^{n-i}} p(r_1,\ldots, r_{i-1},X ,x_{i+1}, \ldots, x_n),
\]
of degree $d_i \leq \deg_{X_i} p(X_1,\ldots, X_n)$, where $r_1, \ldots, r_{i-1}$ are the randomnesses received in the previous rounds. (In the first round $i=1$ there are no previous randomnesses, and $p(r_1,\ldots, r_{i-1},X $, $x_{i+1}, \ldots, x_n)$ is meant to denote $p(X,x_2,\ldots, x_n)$.)
%(Again, the computation is done by interpolation from the values over a set $D_i\supseteq \{\pm 1\}$ of size $|D_i| = d_i + 1$.)
The prover sends the coefficients of $s_{i}(X)$ to the verifier, which checks whether the received polynomial $s_i(X)$ is in fact of the expected degree and that
\[
s_{i-1}(r_{i-1}) = s_{i}(+1) + s_{i}(-1).
\] 
(Again, in the first round $i=1$ there is no $r_0$, and the verifier checks wheather $s_0 = s_1(+1) + s_1(-1)$.) 
If so, the verifier samples random challenge $r_i\sample F$ uniformly from $F$ and sends it to the prover.
\end{itemize}
After these rounds the verifier checks that $s_n(r_n) = p(r_1,\ldots, r_n)$. 
If so, the verifier accepts (otherwise it rejects).  
\end{protocol}


Soundness of the sumcheck protocol is proven by a repeated application of the Schwartz-Zippel lemma. 
We omit a proof, and refer to \cite{sumcheck} or \cite{SumcheckThaler}. 
\begin{thm}[\cite{sumcheck}]
The sumcheck protocol (Protocol \ref{p:Sumcheck}) has soundness error
\begin{equation}
\label{e:SumcheckSoundness}
\varepsilon_{sumcheck} \leq \frac{1}{|F|}\cdot \sum_{i=1}^n \deg_{X_i} p(X_1,\ldots, X_n).
\end{equation}
\end{thm}

The sumcheck protocol is easily extended to the sumcheck for a batch of polynomials $p_i(X_1,\ldots, X_n)$, $i=0, \ldots, L$, by letting the verifier sample a random vector $(\lambda_1,\ldots, \lambda_L)\sample F^L$, and a subsequent sumcheck protocol for the random linear combination
\[
\bar p (X_1, \ldots, X_n) = p_0(X_1,\ldots, X_n) + \sum_{i=1}^{L} \lambda_i \cdot p_i(X_1,\ldots, X_n).
\]
The soundness error bound increases only slightly,
\begin{equation}
\label{e:BatchSumcheckSoundness}
\varepsilon_{sumcheck} \leq \frac{1}{|F|}\cdot \left(1 + \sum_{i=1}^n \deg_{X_i} p(X_1,\ldots, X_n)\right).
\end{equation}

%\begin{rem}
\subsubsection{Computational cost}

Let us discuss the prover cost of the sumcheck protocol for the case that  $p(\vec X) = p(X_1,\ldots, X_n)$ is of the form
\[
p(\vec X) = Q(w_1(\vec X), \ldots, w_\nu(\vec X)),
\]
with each $w_i(\vec X)\in F[X_1,\ldots, X_n]$ being multilinear, and 
\[
Q(Y_1,\ldots, Y_\nu) = \sum_{(i_1,\ldots, i_\nu)\in \{0,1\}^\nu} c_{i_1,\ldots, i_\nu} \cdot Y_1^{i_1}\cdots Y_\nu^{i_\nu}
\]
 is a multivariate polynomial having (a typically low) absolute degree $d$.
We denote the arithmetic complexity, i.e. the number of field multiplications $\mathsf M$, substractions and additions $\mathsf A$ to evaluate $Q$ by $|Q|_\textsf M$ and $|Q|_\textsf A$, respectively.
(For simplicity we count substractions as additions.) 
Each of the univariate polynomials $s_i(X)$, $i=1,\ldots, n$, is of degree at most $d$ the absolute degree of $Q$, and is computed from its values over a set $D\supseteq \{\pm 1\}$ of  size $|D| = d + 1$.
In each step $i=1,\ldots, n$, the values of $s_i(z)$ for $z\in D$ are obtained by linear interpolation of the domain evaluations of each
\[
w_j (r_1,\ldots, r_{i-1}, \pm 1, X_{i+1}, \ldots, X_n)
\]
over $H_{i}=\{\pm 1\}^{n-i}$ as given from the previous step, to the domain evaluation
\[
w_j (r_1,\ldots, r_{i-1}, z, X_{i+1}, \ldots, X_n), 
\]
the values of which are used for computing $s_i(z) = \sum_{(x_{i+1},\ldots, x_n)\in H_{i}} Q(r_1,\ldots, r_{i-1}, z, x_{i+1}, \ldots, x_n)$.
Given the random challenge $r_i$ from the verifier, the domain evaluation of each   
\[
w_j(r_1,\ldots, r_{i-1}, r_i, X_{i+1},\ldots, X_n)
\]
is computed by another linear interpolation.
Linear interpolation costs $|H_i|$ multiplications and the same number of additions/substractions for each multilinear polynomial, the values of $Q$ are obtained within $|Q|_\textsf M \cdot \textsf{M} + |Q|_\textsf A \cdot \textsf A$.  
In terms of field multiplications $\mathsf M$ and substractions/additions $\mathsf A$, step $i$ consumes 
% Interpolation of m multilinear polynomials for |D_i| - 2 + 1 many points:
%     m*  |H_i| S for the differences
%     m * (|D_i| - 2) * |H_i| (M + A) for the domain evals for every z in D_i \setminus\{\pm 1\} 
%     m * |H_i| (M + A) for the domain eval at r_i
% Domain evaluation for Q at each point in D_i
%   |D_i| * |H_i| * (Q_M * M + Q_S * S + Q_A * A)
% Sum over |H_i| for Q at each point z in D_i
%  |D_i| * |H_i| * A
\begin{equation*}
\nu\cdot |H_i|\cdot \textsf A + \nu \cdot (|D| - 1)\cdot |H_{i}| \cdot (\textsf M + \textsf A)
+  |D|\cdot |H_{i}| \cdot ( |Q|_\textsf M \cdot \textsf{M} +  |Q|_\textsf A \cdot \textsf A) 
\\
+ |D|\cdot |H_{i}| \cdot \textsf A,
%\\
%< |D|  \cdot |H_{i}| \cdot \big((m + |Q|_M)\cdot\textsf M +  (m+ |Q|_\textsf S) \cdot\textsf S + (m + |Q|_\textsf A + 1)\cdot \textsf A\big).
\end{equation*}
where the last term is for the domain sums.
Since $\sum_{i=1}^{n} |H_{i}| = |H| - 1$, the overall cost for the prover is bounded by 
%\begin{equation}
%\label{e:sumcheck:cost:precise}
%|H|\cdot \left(1-\frac{1}{|H|}\right)\cdot \big( (d\cdot m + (d+1)\cdot |Q|_\textsf M)\cdot\textsf M +
% (m + (d + 1)\cdot |Q|_\textsf S) \cdot\textsf S +
%(d\cdot m + (d+1)\cdot (|Q|_\textsf A + 1))\cdot\textsf A
%\big).
%\end{equation}
\begin{equation}
\label{e:sumcheck:cost:precise}
|H|\cdot \left(1-\frac{1}{|H|}\right)\cdot \big( (d\cdot \nu + (d+1)\cdot |Q|_\textsf M)\cdot\textsf M +
(d+1)\cdot (\nu + |Q|_\textsf A) + d)\cdot\textsf A
\big).
\end{equation}
We shall use the simplified formula
\begin{equation}
\label{e:sumcheck:cost}
|H|\cdot (d+1)\cdot\big( (\nu +  |Q|_\textsf M)\cdot\textsf M +
 (\nu + |Q|_\textsf A)\cdot\textsf A
\big)
\end{equation} 
for the operation counts of our lookup protocol.




\section{Lookups based on the logarithmic derivative}
\label{s:lookups}

Assume that $F$ is a finite field, and that $f_1, \ldots, f_M$ and  $t: H\rightarrow F$ are functions over the Boolean hypercube $H=\{\pm 1\}^n$. 
By Lemma \ref{lem:batchsetmembership}, it holds that $\bigcup_{i=1}^M \{f_i(\vec x)\}_{\vec x\in H}\subseteq \{t(\vec x)\}_{\vec x\in H}$ as sets, if and only if there exists a function $m: H\rightarrow F$ such that
\begin{equation}
\label{e:lookup:fractional:identity}
\sum_{\vec x\in H} \sum_{i=1}^M \frac{1}{X + f_i(\vec x)} = \sum_{\vec x\in H} \frac{m(\vec x)}{X + t(\vec x)},
\end{equation}
assuming that the characteristic of $F$ is larger than $M$ times the size of the hypercube.
If $t$ is injective (which is typically the case for lookup tables) then $m$ is the multiplicity function, counting the number of occurences for each value $t(\vec x)$ in $f_1,\ldots, f_M$ altogether, i.e.
$m(\vec x) = m_f(t(\vec x)) = \sum_{i=1}^M|\{\vec y \in H: f_i(\vec y) = t(\vec x))|$.
If $t$ is not one-to-one, we set $m$ as the \textit{normalized} multiplicity function 
\begin{equation}
\label{e:lookup:m}
m(\vec x) = 
\frac{m_f(t(\vec x))}{m_t(t(\vec x))} = \frac{ \sum_{i=1}^M |\{\vec y \in H: f_i(\vec y) = t(\vec x))|}{ |\{\vec y \in H: t(\vec y) = t(\vec x))|}.
\end{equation}
%The plot for proving that $\bigcup_{i=1}^M \{f_i(\vec x)\}_{\vec x\in H}\subseteq \{t(\vec x)\}_{\vec x\in H}$ is as follows.
Given a random challenge $x\sample F$ from the verifier, the prover shows that the rational identity \eqref{e:lookup:fractional:identity} holds at $X= x$, i.e.
\begin{equation}
\label{e:lookup:fractional:sumcheck}
\sum_{\vec x\in H} \sum_{i=1}^M \frac{1}{x + f_i(\vec x)} -  \frac{m(\vec x)}{x + t(\vec x)} = 0,
\end{equation}
whenever evaluation is possible. 
However, in order to apply the sumcheck protocol we need to turn the fractional expression into a polynomial one.
For that, the prover splits the sum into partial sums of (roughly) the same number of terms $\ell$, and provides multilinear helper functions for each sum.
These helper functions are subject to a domain identity of algebraic degree essentially equal to  the number of reciprocal terms in the sum.
In practice one chooses $\ell$ so that the prover time is minimal for the given number of columns $M$.
That optimum depends on the used polynomial commitment scheme. 
The costlier the computation of a commitment, the higher the algebraic degree for the domain identity of the helper function can be. 

In Section \ref{s:lookup:small} we describe our batch-column lookup, Protocol \ref{prot:lookup}, and we sketch its soundness analysis in Section \ref{s:lookup:soundness}. 
In the following Section \ref{s:lookup:cost} we give detailed operations counts for its oracle prover, which we then use to estimate the optimal choice of $\ell$ assuming a commitment scheme which uses an ordinary sized elliptic curve\footnotemark.
Eventually, we point out the generalization of the lookup argument to vector-valued tables.
\footnotetext{%
By ordinary sized we mean a $128$ bit secure curve over a $256$ bit large base field.
This covers both KZG-like commitments in Barreto-Naehrig curves, and IPA commitments in an ordinary elliptic curve.
}

%In Section \ref{s:lookup:optimalM} we estimate the optimal $K$, assuming a commitment scheme in an ordinary sized elliptic curve\footnotemark.
%
%Our building block lookup protocol, Protocol \ref{prot:lookup}, saves as much commitments as possible.
%The prover provides a single multilinear helper function for the entire sumcheck expression, at the cost of an overall domain identity of absolute degree $M$, roughly. 
%In Section \ref{s:lookup:soundness} we sketch its soundness, and
%We stress the fact that our notion of optimality refers to the minimal cost per column of Protocol \ref{prot:lookup}, which is a somewhat simplified cost measure.
%It is accurate when processing larger batches of columns by parallel invocations of the protocol on $M_{opt}$ columns each, a strategy which we consider for simplicity of the presentation.
%In practice one uses a single multiplicity function and a single sumcheck across several invocations of the protocol. 
%

\subsection{The protocol}
\label{s:lookup:small}

Let $\ell$ be the chosen sum size, $[0,M] = \bigcup_{k=1}^K I_k$ the decomposition of $[0,M]$ into $K=\ceil{\frac{M+1}{\ell}}$ subintervals $I_k = [(k-1)\cdot \ell, k\cdot\ell)\cap [0,M]$, $k=1,\ldots, K$. 
Let 
\begin{equation}
\label{e:lookup:hk}
h_k(\vec x) = \sum_{i\in I_k} \frac{m_i(\vec x)}{\varphi_i(\vec x)}, \quad k= 1,\ldots, K,
\end{equation}
be the respective partial sum of consecutive terms in the overall expression $\frac{m(x)}{x+t(\vec x)} - \frac{1}{x + f_1(\vec x)} - \ldots - \frac{1}{x + f_M(\vec x)}$, where we used the notation 
\begin{align*}
m_{0}(\vec x) = m(\vec x), \quad &\varphi_{0}(\vec x) = x + t(\vec x),
\\
m_{i}(\vec x) = -1, \quad &\varphi_{i}(\vec x) = x + f_i(\vec x),  \quad  i=1,\ldots, M.
\end{align*}
The prover provides the oracles for $h_1, \ldots, h_k$, subject to 
\[
\sum_{\vec x \in H} h_1(\vec x) + \ldots + h_{K}(\vec x) = 0,
\] 
and the domain identities
\begin{equation}
\label{e:lookup:h:identity}
h_k(\vec x) \cdot \prod_{i\in I_k} \varphi_i(\vec x) = \sum_{i\in I_k} m_i(\vec x) \cdot \prod_{j\in I_k\setminus\{i\}} \varphi_j(\vec x)
\end{equation}
over $H$, the latter are reduced to sumchecks by applying the Lagrange kernel $L_H(\:.\:, \vec z)$ at a randomly chosen $\vec z\sample F^n$.
All sumchecks are then combined into a single one, using random scalars $\lambda_1,\ldots, \lambda_K\sample F$.

\begin{protocol}[Batch-column lookup over $H=\{\pm 1\}^n$]
\label{prot:lookup}
Let $M$ be an integer, and $F$ a finite field with characteristic $p > M\cdot 2^n$. 
Fix any integer $\ell$, $1\leq \ell\leq M+1$, and let $K=\ceil{\frac{M+1}{\ell}}$.
Given any functions $f_1, \ldots, f_M, t :H\rightarrow F$ on the boolean hypercube $H=\{\pm 1\}^n$, the Lagrange IOP for that $\bigcup_{i=1}^M\{f_i(\vec x) : \vec x\in H\}\subseteq \{t(\vec x) : \vec x\in H\}$ as sets is as follows.

\begin{enumerate} 
\item
The prover determines the (normalized) multiplicity function $m:H\rightarrow F$ as defined in \eqref{e:lookup:m},
and sends the oracle for $m$ to the verifier.
The verifier answers with a random sample $x\sample F\setminus \{- t(\vec x) : \vec x\in H\}$. 

\item
\label{i:lookup:step1}
Given the challenge $x$ from the verifier, the prover computes the values over $H$ for the partial sums $h_1(\vec x), \ldots, h_K(\vec x)$ as defined above, and sends their oracles to the verifier.

\item
\label{i:lookup:step2}
The verifier responds with a random vector $\vec z \sample F^n$ and random batching scalars $\lambda_1,\ldots, \lambda_K\sample F$.
Now, both prover and verifier engage in the sumcheck protocol (Protocol \ref{p:Sumcheck}) for 
\begin{align*} 
%\label{e:sumcheckh}
\sum_{\vec x \in H} Q(L_H(\vec x, \vec z),  m(\vec x),  \varphi_0(\vec x), \ldots, \varphi_M(\vec x),  h_1(\vec x),\ldots, h_K(\vec x))&= 0,
\end{align*}
where  %$Q$ is the degree $M+2$ polynomial
\begin{equation}
\label{e:lookup:Q}
Q(L, m, \varphi_0,\ldots, \varphi_M,  h_1, \ldots, h_K) =   
\sum_{k=1}^K h_k  + L \cdot \lambda_k \cdot \left(h_k \cdot \prod_{i\in I_k} \varphi_i - \sum_{i\in I_k} m_i\cdot \prod_{j\in I_k\setminus\{i\}} \varphi_j \right),
\end{equation}
with $m_0 = m$, and all other $m_i = -1$, $i=1,\ldots, M$.
The sumcheck protocol outputs the expected value $v$ for the multivariate polynomial 
\begin{equation}
\label{e:lookup:QinX}
\begin{aligned}
Q(L_H(\vec X, \vec z), m(\vec X), \varphi_0(\vec X),\ldots, \varphi_M(\vec X),  h_1(\vec X),\ldots, h_K(\vec X))
\end{aligned}
\end{equation}
at $\vec X=\vec r$ sampled by the verifier in the course of the protocol.

\item
The verifier queries $[m], [t], [f_1], \ldots, [f_M], [h_1], \ldots, [h_K]$ for their inner product with $L_H(\:.\:,\vec r)$, and uses the answers 
to check whether \eqref{e:lookup:QinX} equals the expected value $v$ at $\vec X = \vec r$. 
(The value $L_H(\vec r, \vec z)$ is computed by the verifier.)
\end{enumerate}
\end{protocol}

\begin{rem}
\label{rem:lookup:completeness}
We imposed the condition $x\notin \{- t(\vec x)\}_{\vec x \in H}$ merely for completeness. 
However in some applications it may be not be desirable, or even not possible, to sample $x$ from outside the range of $t$.
There are several ways to handle this.
One can simply omit the constraint on $x$, letting the verifier sample $x\sample F$ and the prover set $h_0$ arbitrary whenever \eqref{e:lookup:hk} is not defined.
This comes at no extra cost, but the obtained protocol is only overwhelmingly complete. 
That is, with a probability of at most $\frac{|H|}{|F|}$ in the verifier randomness $x$, the honest prover does not succeed.
In practice this is often considered acceptable, and many lookup implementations have a non-zero completeness error. 
Whenever this is not acceptable, one may modify the domain identity for $h_0$ to 
\begin{equation}
\label{e:lookup:h:identity:complete}
\left(h_0 \cdot \prod_{i\in I_0}\varphi_i - \sum_{i \in I_0} m_k\cdot \prod_{j\neq i} \varphi_j\right)\cdot   \varphi_0  = 0
\end{equation}
over $H$, which imposes no condition on $h_0$ whenever $\varphi_0(\vec x)= 0$. 
However, this approach comes at the cost of a slight increase  in the degree of $Q$ with respect to $\varphi_0$.
\end{rem}

Let us point out two variations of Protocol \ref{prot:lookup}.
In the single-column case $M=1$ the lookup argument can be turned into a multiset check for the ranges of $f_1$ and $t$, by setting $m$ as the constant function $m(\vec x) = 1$.
In this case only $h_0$ needs to be provided by the prover.
More interestingly, Protocol \ref{prot:lookup} is easily extended to a proof of range equality, showing that $\bigcup_{i=1}^M \{f_i(\vec x)\}_{\vec x\in H} = \{ t(\vec x)\}_{\vec x\in H}$ as sets.
For this the prover additionally shows that $m \neq 0$ over $H$, which is done by providing another auxiliary function $g: H\rightarrow F$ subject to $g\cdot m = 1$ over $H$.
However, we are not aware of any application of this fact.





\subsection{Soundness}
\label{s:lookup:soundness}

The soundness analysis of Protocol \ref{prot:lookup} is a straight-forward application of the Schwartz-Zippel lemma and the Lagrange-query to point-query correspondence stated by Lemma \ref{lem:Lagrange}.
We merely sketch it.
The univariate rational lookup identity \eqref{e:lookup:fractional:identity} is turned into a polynomial identity of degree at most $|H|\cdot (M+1) - 1$ by multiplying it with the common denominator  
\begin{equation}
\label{e:lookup:common:denominator}
\prod_{\vec x\in H} (X + t(\vec x)) \cdot \prod_{i=1}^M (X + f_i(\vec x)).
\end{equation}
Since we sample $x$ from a set of size at least $|F|-|H|$, the soundness error this step is at most 
\begin{equation}
\label{e:lookup:epsilon1}
\varepsilon_1 \leq \frac{(M+1)\cdot |H| - 1 }{|F|-|H|}.
\end{equation}
The soundness error due to the reduction of the domain identities  \eqref{e:lookup:h:identity}  to the Lagrange kernel based sumcheck is altogether
\[
\varepsilon_2 \leq \frac{K}{|F|},
\]
as scalar products with the Lagrange kernel translate to point evaluation of the multilinear extension.
Finally, the batching step of the $K+1$ sumchecks has soundness error $\varepsilon_3 = \frac{1}{|F|}$.
This yields the following theorem. 

\begin{thm}
\label{thm:lookup:soundness}
 The interactive oracle proof described Protocol \ref{prot:lookup} has soundness error
\[
\varepsilon < \frac{(M+1)\cdot |H| - 1 }{|F|-|H|} + \frac{K+1}{|F|} + \varepsilon_{sumcheck},
\]
where $\varepsilon_{sumcheck}$ is the soundness error of the sumcheck argument \eqref{e:SumcheckSoundness} over $H$ for a multivariate polynomial in $M+4$ variables with maximum individual degree $M+3$.
\end{thm}
%\begin{rem}
%The $\bigO M$-variant described in Section \ref{prot:lookup} has same soundness error, with $\varepsilon_{sumcheck}$ being the soundness error of the sumcheck argument over the extended hypercube of size $M\cdot |H|$ for a multivariate polynomial in $\nu=5$ variables and maximum individual degree $4$. 
%\end{rem}


\subsection{Computational cost and optimal sum size}
\label{s:lookup:cost}

%Let us determine the prover cost of Protocol \ref{prot:lookup}.
The polynomial $Q$ from \eqref{e:lookup:Q} has $\nu= M + K + 3$ variables, and absolute degree $d = \ell + 2$.
Let us describe a domain evaluation strategy for $Q$ which uses of batch inversion. 
This strategy allows us to evaluate $Q$ much more efficiently than using \eqref{e:lookup:Q}, but demands a modification of the sumcheck operation count formula \eqref{e:sumcheck:cost}.
%Let us elaborate an evaluation strategy for $Q$, which makes use of 
% Arithmetic complexities, using the inverses of phi_1, ..., phi_{M-1}
% 	M  - 1 multiplications for p = phi_1 * ... * phi_{M-1} * phi_M
% 	M - 1 multiplications to obtain the partial products p_i = \prod_{j\neq i} \phi_j for i=1,..,M-1. (the product p_M is already known)
%	2 muls for m * p + tau * (h * p + \sum_{i} p_i)
%	another 2 muls for the product with L and lambda.
Assume that in each group $I_k$ the inverses of $\varphi_i$ are given, except for one distinct $i_k\in I_k$.
Then we may evaluate the domain identity terms in $Q$ by the fractional representation
\begin{equation*}
h_k \cdot \prod_{i\in I_k} \varphi_i - \sum_{i\in I_k} m_i\cdot \prod_{j\in I_k\setminus\{i\}} \varphi_j = 
 \prod_{i\in I_k\setminus\{i_k\}} \varphi_j \cdot \left( \varphi_{i_k} \cdot \left( h_k - \sum_{i\in I_k\setminus\{i_k\}} \frac{m_i}{\varphi_i}\right) - m_{i_k} \right)
%\prod_{i\in I_k} \varphi_i \cdot \left( h_k - \sum_{i\in I_k} \frac{m_i}{\varphi_i}\right).
\end{equation*}
For $k\geq 2$, all involved $m_i = -1$, and we have 
\begin{equation}
\label{e:lookup:cost:k2}
 \prod_{i\in I_k\setminus\{i_k\}} \varphi_j \cdot \left( \varphi_0 \cdot \left( h_k + \sum_{i\in I_k\setminus\{i_k\}} \frac{1}{\varphi_i}\right) + 1 \right),
\end{equation}
regardless of the choice of $i_k$. 
For the first group $k=1$, we chose $i_1 = 0$, so that 
\begin{equation}
\label{e:lookup:cost:k1}
\prod_{i\in I_1\setminus\{0\}} \varphi_j \cdot \left( \varphi_0 \cdot \left( h_k + \sum_{i\in I_1\setminus\{0\}} \frac{1}{\varphi_i}\right) - m \right).
\end{equation}
In both cases \eqref{e:lookup:cost:k1} and \eqref{e:lookup:cost:k2} the evaluation costs are $\ell\cdot \mathsf M + \ell \cdot\mathsf A$, counting substractions as additions. 
Henceforth $Q$ is evaluated in overall
\begin{equation}
K \cdot (\ell  +2) \cdot \mathsf M + K\cdot (\ell+1)\cdot \mathsf A.
\end{equation}
Now, to attribute the inverses in formula \eqref{e:sumcheck:cost}, we increase the multiplicative complexity by $3\cdot (M - K)$, which represents the fractional cost of the batch inversion\footnotemark of all the $\varphi_i$ except one in each of the groups. 
\footnotetext{%
Batch, or Montgomery inversion, of a sequence $(a_i)_{i=1}^N$ computes the inverses of $a_i^{-1}$ by recursively computing the cumulative products $p_i = a_1\cdot\ldots \cdot a_n$, $i=0,\ldots,n$, then calculating their inverses $q_i = \frac{1}{p_i}$ in a reverse manner   
starting with $q_n = \frac{1}{p_n}$, and putting $q_{i-1} = q_i \cdot a_i$, where $i$ goes from $n$ down to $1$. 
The inverses are then derived via $a_i^{-1} = p_{i-1}\cdot q_{i}$, where $p_0:=1$.
The overall cost of the batch inversion is $3\cdot (N-1)$ multiplications and a single inversion.
}%
This yields the following equivalent complexities
 \[
|Q_\mathsf M| = 3\cdot M + K\cdot (\ell - 1), \quad |Q_\mathsf A| = K\cdot (\ell+1),
\]
which we may plug into formula \eqref{e:sumcheck:cost}.

The prover cost of Protocol \ref{prot:lookup} is as follows:
Given the values of $t$ and $f_1, \ldots, f_M$  over $H$, computing $\varphi_0 = x + t$ and $\varphi_1 = x + f_1, \ldots, \varphi_M = x + f_M$ costs $|H|\cdot (M+ 1) \cdot\mathsf A$, and their reciprocals $\frac{1}{\varphi_0},  \ldots, \frac{1}{\varphi_M}$ are obtained within $3\cdot |H| \cdot (M+1) \cdot\mathsf M$, using batch inversion. 
With these reciprocals we obtain the values for $h_1, \ldots, h_K$ within overall $|H|\cdot (\mathsf M + K\cdot (\ell - 1) \cdot \mathsf A)$. 
By the remark following Lemma \ref{lem:Lagrange}, the values for $L_H(\vec X, \vec y)$ over $H$ are computed in $|H|\cdot (\mathsf M + \mathsf A)$ operations. 
Hence the total cost of the preparation phase is
\begin{equation*}
%\label{e:lookup:cost:prep}
|H|\cdot ((3\cdot M + 5)\cdot\mathsf M + (M + K\cdot (\ell-1)+ 2) \cdot\mathsf A.
\end{equation*}
According to \eqref{e:sumcheck:cost} the sumcheck costs  
\begin{equation*}
%\label{e:lookup:cost:sumcheck}
|H| \cdot  (\ell + 3)\cdot \big( (4\cdot M+ 3 + \ell \cdot K)  \cdot\textsf M + 
	(M + 3  + (\ell + 2)\cdot K) \cdot \textsf A\big).
\end{equation*}
However, as we may reuse the reciprocals of $\varphi_0,  \ldots, \varphi_{M}$ in the first step of the sumcheck, we may correct the sumcheck cost by substracting $|H|\cdot 3\cdot (M - K)$. 
The total costs of the oracle prover are 
\begin{itemize}
\item
arithmetic costs of
\begin{equation}
\label{e:lookup:cost}
|H| \cdot \left(K+ 5  + (\ell + 3)\cdot (4\cdot M+ 3 + \ell \cdot K) \right) \cdot \mathsf M,
\end{equation}
neglecting field additions and substractions, and
\item 
oracle costs of 
\begin{equation}
K+1 = \ceil{\frac{M+1}{\ell}} + 1
\end{equation} 
oracles of size $|H|$.
\end{itemize}
%The cost is $\bigO{|H|}$ but depends quadratically in $M$ the number of columns to be looked up. 
%This quadratic occurence is due to the fact that both, the number of function as well as the degree of $Q$ grow linearly in $M$. 



\subsubsection{The optimal choice of $\ell$}

Let us discuss the concrete impact of the partial sum size $\ell$ on the overall prover cost. 
%First of all, let us illustrate the two extreme cases. 
If $\ell = 1$, then each reciprocal is given a helper function, resulting in $M + 1$ additional commitments (besided the one for $m$) but a very low degree of $Q$, $d= 3$  .
Increasing $\ell$ reduces the number of commitments, but comes at the cost of a growing degree.
The most extreme case is $\ell = M+1$, where the prover provides a single additional commitment besides $m$, but needs to cope with a degree of $d= M + 3$, resulting in sumcheck costs which are quadratic in $M$.
The optimal choice for $\ell$ is between these two cases, but depends on the concrete polynomial commitment scheme.
We consider elliptic curve based commitments. 
To take the commitment costs into account we rely on the benchmarks from Table \ref{tab:pippenger} which measure the equivalent number of field multiplications for a multi-scalar multiplication in an ordinary-sized elliptic curve of $128$ bit security.


\begin{table}
\caption{%
Benchmark of Halo2's Pippenger multi-scalar multiplication in the Pallas curve, varying the number $N$ of scalars.
The benchmarks where done on a AMD Ryzen 7 PRO 4750U, 32GB RAM DDR4,  restricting to a single core.
}
\label{tab:pippenger}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
$\log N$ &  Pippenger of size $N$ & $2^8\cdot N$ field mult. & equivalent field mult.
\\\hline
12 & $46.010$ ms &  $21.11$ ms & $N\cdot 557\cdot \mathsf M$
\\
14 & $153.67$ ms & $84.78$ ms & $N\cdot 464 \cdot \mathsf M$
\\
16 & $522.13$ ms & $169.70$ ms & $N\cdot 394\cdot\mathsf M$
\\
18 &       $1.869$  ms  & $679.27$ ms & $N\cdot 351\cdot\mathsf M$
\\\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption{%
An estimate of the partial sum size $\ell$ in Protocol \ref{prot:lookup}, which minimizes the prover cost. 
The numbers are based on the operation counts \eqref{e:lookup:cost} for the oracle prover and the benchmarks for a multi-scalar multiplication in the Pallas curve, Table \ref{tab:pippenger}.
}
\label{tab:lookup:optimal}
\vspace*{0.5cm}
\centering
\begin{tabular} {|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{$\log N$} & \multicolumn{4}{c|}{$\ell$} & \multicolumn{4}{c|}{field mult. per col.} 
\\\cline{2-9}
& $M=10$ & $M=20$ & $M=70$ & $M=\infty$ & $M=10$ & $M=20$ & $M=70$ & $M=\infty$
\\\hline
12 & 11 & 11 & 11 & 11 & $N\cdot 188$ & $N\cdot 158$ &  $N\cdot 135$ &  $N\cdot 120$
\\
14 & 11 & 11 & 9 & 9 & $N\cdot 170$ & $N\cdot 144$ &  $N\cdot 120$ &  $N\cdot 112$
\\
16 & 11 & 11 & 9 & 9 & $N\cdot 155$ & $N\cdot 132$ &  $N\cdot 112$&  $N\cdot 104$
\\
18 & 11 & 11 & 8 &  8 &  $N\cdot 147$ & $N\cdot 126$  &  $N\cdot 106$ &  $N\cdot 99$
\\\hline
\end{tabular}
\end{table}




%%
%% Poseidon hash comparison
%%
%As a lower bound for elliptic curve based schemes, we take the number of field operations for $x^5$-Poseidon with rate $r=2$, capacity $c=1$, $R_F=8$ full rounds and $R_p=57$ partial rounds. (These are the parameters from \cite{Poseidon} for a security level of $128$ bits over a $255$ bit large base field.) 
%Each permutation, which processes $r=2$ many elements costs
%$
%604\cdot\mathsf M + 591\cdot\mathsf A, 
%$ 
%using the optimized evaluation strategy from Appendix B in \cite{Poseidon}.
%Hence computing the hash of two functions over $H$ costs
%\[
%|H|\cdot (604\cdot\mathsf M + 591\cdot\mathsf A). 
%\]
%Therefore a $M$ single-column lookups cost 
%\begin{equation}
%\label{lookup:cost:Mcolumn:naive}
%|H|\cdot ((604\cdot M + 42)\cdot\mathsf M + 10\cdot\mathsf S + (591\cdot M + 30)\cdot\mathsf A),
%\end{equation}
%and the batch-column lookup for $M$ columns
%\begin{equation}
%\label{lookup:cost:Mcolumn:batch}
%|H| \cdot  \big((5\cdot M^2 + 19 \cdot M + 1226) \cdot\textsf M 
%+ (2\cdot M + 8)\cdot\textsf S 
%+ (2\cdot M^2 + 13\cdot M + 621) \cdot \textsf A\big).
%\end{equation}
%Comparing \eqref{lookup:cost:Mcolumn:naive} and \eqref{lookup:cost:Mcolumn:batch} yields  a maximum speedup by a factor of $5$ for $M=11$, more than double as fast for $3\leq M\leq 58$, and a break-even point at about $M=124$ columns.

%\newpage
\subsection{Vector-valued lookups}
\label{s:pa:Generalizations}

As Plookup, Protocol \ref{prot:lookup} is easily generalized to functions with multilinear values, 
\begin{align*}
t(\vec x) &= \sum_{(j_1,\ldots, j_k)\in\{0,1\}^k} t_{j_1,\ldots, j_k}(\vec x)\cdot Y_1^{i_1}\cdots Y_k^{j_k},
\\
f_i(\vec x) &= \sum_{(j_1,\ldots, j_k)\in\{0,1\}^k} f_{i, j_1,\ldots, j_k}(\vec x)\cdot Y_1^{i_1}\cdots Y_k^{j_k},
\end{align*}
$i=1,\ldots, M$, without changing the soundness error bound from Theorem \ref{thm:lookup:soundness}.
As $F[X, Y_1,\ldots, Y_k]$ is a unique factorization domain, and polynomials of the form $X -  \sum_{(i_1,\ldots, i_k)\in\{0,1\}^k} c_{i_1,\ldots, i_k}\cdot Y_1^{i_1}\cdots Y_k^{i_k}$ are irreducible, we may apply Lemma \ref{lem:batchsetmembership} to see that $\bigcup_{i=1}^M \{f_i(\vec x)\}_{\vec x\in H}\subseteq \{t(\vec x)\}_{\vec x\in H}$ as sets in the rational function field $F(X, Y_1,\ldots, Y_k)$, if and only if there exists a function $m: H\rightarrow F$ such that
\begin{equation*}
%\label{e:lookup:fractional:identity:general}
\sum_{\vec x\in H} \sum_{i=1}^M \frac{1}{X + f_i(\vec x)(\vec Y)} = \sum_{\vec x\in H} \frac{m(\vec x)}{X + t(\vec x)(\vec Y)}.
\end{equation*}
The only change to Protocol \ref{prot:lookup} is that the verifier now samples $x$ from $F$ and $\vec y = (y_1,\ldots, y_k)$ from $F^k$, and continues with $x - f_i(\vec x)$ and $x - t(\vec x)$ replaced by $x - f_i(\vec x)(\vec y)$ and $x - t(\vec x)(\vec y)$.
%As the individual degrees with respect to $Y_1$, \ldots, $Y_k$ are again bounded by $|H|$, the soundness error does not change.
%Alternatively, at the cost of a slight increase of the soundness error one can choose $f, g: H\longrightarrow F[X]$ with values being polynomials of degree at most $k-1$ . 
%\end{rem}


\newpage
\section{Multivariate Plookup}
\label{s:hyperplonk}

In this section we sketch batch-column lookups using the Plookup strategy and the multivariate time shift introduced by Hyperplonk \cite{Hyperplonk}. 
%We  give detailed operation counts and compare with the lookup argument from Section \ref{s:lookups}.
The time shift $T: H \rightarrow H$ on the boolean hypercube $H = \{\pm 1\}^n$ is derived from the multiplication by a primitive root in $GF(2^n)$, 
\[
T(x_1, \ldots, x_n) = \frac{1 + x_n}{2} \cdot (1, x_1,\ldots, x_{n-1}) + \frac{1 - x_n }{2} \cdot (- 1,(-1)^{1 - c_1}\cdot x_1,\ldots, (-1)^{1 - c_{n - 1}}\cdot x_{n-1}),
\]  
where the $c_i\in\{0,1\}$ are the coefficients of a primitive polynomial $1 + \sum_{i=1}^{n-1} c_i\cdot X^i + X^n$ over $GF(2)$.
The time shift acts transitively\footnotemark on the punctuated hypercube 
\footnotetext{%
As a group automorphism it has $\vec 1=(1,1,\ldots, 1)$ as a fixed point.
}%
\[
H' = H\setminus\{\vec 1\},
\] 
and more importantly, evaluations of a shifted function $f(T(\vec x))$ can be simulated from two evaluations of $f$ by
\begin{equation*}
f(T(x_1, \ldots, x_n)) =  \frac{1 + x_n}{2} \cdot f(1, x_1,\ldots, x_{n-1}) + \frac{1 - x_n }{2} \cdot f(- 1,(-1)^{1 - c_1}\cdot x_1,\ldots, (-1)^{1 - c_{n - 1}}\cdot x_{n-1}).
\end{equation*}

%Using the time shift $T$ allows for the same strategy for the univariate Plookup argument. 
%The argument is based on the fact that, given two sequences of field elements $(a_i)_{i=0}^{N-1}$ and $(t_i)_{i=0}^{N-1}$, then $\{a_i : j= 0,\ldots, N-1\}\subseteq \{t_i : i = 0, \ldots, N-1\}$ as sets, if and only if there exists a sequence $(s_i)_{i=0}^{2N-1}$ of double the size, which satisfies the identity
%\begin{equation}
%\label{e:lookup:Gabizon}
%\prod_{i=0}^{N-1} (X + s_{i}+ s_{i+1\bmod N}\cdot Y)
%= \prod_{i=0}^{N-1}  (X + a_i + a_i\cdot Y)\cdot (X + t_i + t_{i+1\bmod N}\cdot Y).
%\end{equation}
%(The sequence $(s_i)_{i=0}^{2N-1}$ is  the concatenation of the $(a_i)$ and $(t_i)$, ordered by value in the same way as in $t$.)
%We again discuss two approaches\footnotemark for dealing with the grand product obtained from \eqref{e:lookup:Gabizon}.%, when sampling random $(\alpha, \beta)\sample F^2$ for $(X,Y)$.
%\footnotetext{%
%We point out that the presented strategies slightly differ from the one in \cite{Hyperplonk}, which uses the more expensive grand product argument from \cite{Quarks}.
%}
%The main protocol, see Section \ref{s:hyperplonk:small}, applies a batched grand product argument over $H$, independent of $M$ the number of columns, leading to a $\bigO{M^2}$ prover for similar reasons as Protocol \ref{prot:lookup} does.
%The second one proves the grand product argument over an extended hypercube $\bar H$ of size $M\cdot |H|$, which leads to $\bigO{M}$ oracle costs, but has bounded algebraic degree.
%We use this protocol as a reference to determine the optimal batch size.

 
%In terms of performance both are equivalent.)

\subsection{Batch-column Plookup}
\label{s:hyperplonk:small}

Let $t: H'\rightarrow F$ be the lookup table, and $f_i:  H'\rightarrow F$, $i=1,\ldots,M$, the functions subject to the lookup.
Although the functions are defined over the punctuated hypercube $H'$, we assume arbitrary values at $\vec 1$.
(These will be ignored by the lookup argument.)
The prover provides the ordered union of the $f_i$  together with $t$ in a piecewise manner, via the additional functions $s_i: H'\rightarrow F$, $i=1,\ldots, M+1$. 
The \textit{Plookup identity} (in the \cite{LookupsBlog} style) is then
\begin{multline*}
\prod_{\vec x\in H'} \prod_{i=1}^{M} (X + s_{i}(\vec x) + s_{i+1}(\vec x)\cdot Y)\cdot (X + s_{M+1}(\vec x) + s_1(T(\vec x))\cdot Y)
\\
= \prod_{\vec x\in H'} \prod_{i=1}^{M} (X + f_i(\vec x) + f_i(\vec x)\cdot Y)\cdot (X + t(\vec x) + t(T(\vec x))\cdot Y).
\end{multline*}
The identity is reduced to a grand product over $H'$ by random samples $\alpha, \beta\sample F$ for $X$ and $Y$, yielding 
\begin{equation}
\label{e:plookup:grandproduct}
\prod_{\vec x\in H'} h(\vec x) = 1,
\end{equation}
where 
\begin{equation}
\label{e:plookup:product}
h(\vec x) = \frac{\alpha + s_{M+1}(\vec x) + s_1(T(\vec x))\cdot \beta}{\alpha + t(\vec x) + t(T(\vec x))\cdot \beta}\cdot 
\prod_{i=1}^M \frac{(\alpha + s_i(\vec x) + s_{i+1}(\vec x)\cdot \beta)}{
(\alpha + f_i(\vec x) + f_i(\vec x)\cdot \beta)}.
\end{equation}
As in Protocol \ref{prot:lookup}, we control the algebraic degree of the resulting identity for \eqref{e:plookup:grandproduct} by splitting the product \eqref{e:plookup:product} into $K= \ceil{\frac{M+1}{\ell}}$  partial products of size $\ell$, where $1\leq \ell\leq M + 1$. 
For this we use the notation
\begin{align*}
\varphi_0(\vec x) &= \alpha + t(\vec x) + \beta\cdot t(T( \vec x)),
\\
\sigma_0(\vec x) &= \alpha + s_{M+1}(\vec x) + \beta\cdot s_1(T( \vec x)),
\end{align*}
and for $i=1,\ldots, M$,
\begin{align*}
\varphi_{i} (\vec x) &=\alpha +  (1+\beta)\cdot f_i(\vec x),
\\
\sigma_{i}(\vec x) &= \alpha + s_i(\vec x) + \beta\cdot s_{i+1}(\vec x). 
\end{align*}
Then 
\[
h(\vec x) %= \prod_{i=0}^M \frac{\sigma_i(\vec x)}{\varphi_i(\vec x)} 
= \prod_{k=1}^K h_k(\vec x),
\]
with
\[
h_k(\vec x) = \prod_{i\in I_k}  \frac{\sigma_i(\vec x)}{\varphi_i(\vec x)}, 
\]
where $I_k = [(k-1)\cdot \ell, k\cdot\ell) \cap [0, M]$.
For each $k=1, \ldots, K$, the prover computes the cumulative products of the values $h_k(\vec x)$ along the orbit of the time shift $T$ on $H'$, starting with $\phi_k(-\vec 1) = 1$, and setting
\[
\phi_k\big(T^{j}(-\vec 1)\big) = \phi_k\big(T^{j-1}(-\vec 1)\big) \cdot h\big(T^{j-1}(-\vec 1)\big),
\]
for $j= 1, \ldots, |H'|-1$.
At the remaining point $\vec x = \vec 1$ outside $H'$, the prover sets $\phi_k$ to zero.
Correctness of the grand product \eqref{e:plookup:grandproduct} is proven by the domain identities
\begin{align*}
\phi_k(T(\vec x))\cdot \prod_{i\in I_k} \varphi_i(\vec x) - \phi_k(\vec x)\cdot \prod_{i\in I_k}  \sigma_i(\vec x) &= 0,
\end{align*}
and the point identities
\begin{align*}
\phi_{1}(-1) &= 1,
\\
\phi_k(T^{-1}(-\vec 1)) &=  \phi_{(k\bmod K)+1}(-\vec 1), 
\end{align*}
where $k= 1, \ldots, K$.
%which are again turned into sumchecks by using the Lagrangians $L_H(\:.\:,-\vec 1)$ and $L_H(\:.\:, T^{-1}(-\vec 1))$.
These identities are reduced to sumchecks over $H$ by help of the Lagrange polynomials $L_H(\:.\:, -\vec 1)$ and $L_H(\:.\:, \vec y)$, where $\vec y\sample F^n$, and then combined into a single one by random batching scalars $\lambda_1, \ldots, \lambda_K\sample F$.
The resulting overall sumcheck is
\begin{multline*}
\hspace*{-0.5cm}
\sum_{\vec x\in H} Q(L_H(\vec x, \vec y), L_H(\vec x, -\vec 1),  \varphi_{0}(\vec x),\ldots, \varphi_{M}(\vec x), \sigma_{0}(\vec x),\ldots, \sigma_{M}(\vec x),  \phi_1(\vec x),\ldots, \phi_{K}(\vec x), \phi_1(T(\vec x)), \ldots \phi_K(T(\vec x))) 
\\
= 0,
\end{multline*}
where
\begin{equation}
\label{e:lookup:hyperplonk:Q}
\begin{aligned}
Q(L_H, &L, L_T, \varphi_0,\ldots, \varphi_M, \sigma_0,\ldots, \sigma_{M}, \phi_1, \ldots, \phi_K, \phi_{1,T}, \ldots, \phi_{K,T}) 
= 
\\&
L \cdot (\phi - 1) + \sum_{k=1}^K \lambda_k\cdot  L_H \cdot \Big(\phi_{k,T} \cdot \prod_{i\in I_k} \varphi_i
- \phi_k\cdot \prod_{i\in I_k} \sigma_i \Big)+ \mu_k \cdot \left( L_T\cdot \phi_k - L\cdot \phi_{(k \bmod K) + 1}\right). 
\end{aligned} 
\end{equation}
The sumcheck polynomial $Q$ has  absolute degree $d = \ell + 2$, which is the same as in Protocol \ref{prot:lookup}, but  about the double of variables, $\nu = 2\cdot (M+1 + K) + 3$.
Its arithmetic complexities are $|Q_\mathsf M|= K\cdot (2\cdot \ell + 3) + 2$, $|Q_\mathsf A|= 4\cdot K - 1$. 

\subsection{Computational cost and optimal product size}

The prover cost for the Plookup strategy is as follows.
Computing the values for all $\varphi_i,  \sigma_i$ over $H$ consumes overall
\[
|H|\cdot (2\cdot(M+1)\cdot \mathsf M + (3\cdot M+ 4)\cdot \mathsf A),
\]
the quotients $h_k(\vec x)$ are obtained within $|H|\cdot K\cdot (2\cdot(\ell-1) + 4)\cdot \mathsf M$, using batch inversion.
From these values of the $\phi_k(x)$ over $H$ are derived by another  $K\cdot |H|\cdot \mathsf M$.
The domain evaluation for $L_H(\vec X, \vec y)$ is obtained within $|H|\cdot (\mathsf M + \mathsf A)$ operations,
 resulting in the overall preparation cost
\begin{equation}
\label{e:plookup:prep:cost}
|H|\cdot ( (2\cdot(M+1)  + K\cdot  (2\cdot \ell + 3)+1)\cdot \mathsf M + (3 \cdot M+ 5)\cdot \mathsf A).
\end{equation}
According to \eqref{e:sumcheck:cost} the sumcheck costs
\begin{equation}
\label{e:plookup:sumcheck:cost}
|H| \cdot  (\ell + 3)\cdot \big((2\cdot M + 7 + (2\cdot\ell +5)\cdot K)\cdot \mathsf M +  (2\cdot\ell + 6)\cdot (M + 2 + 3\cdot K) \cdot \textsf A\big).
\end{equation}
The total costs of the oracle prover are 
\begin{itemize}
\item
arithmetic costs of
\begin{equation}
\label{e:lookup:cost}
|H| \cdot \left( \left(2\cdot \ell ^2+13\cdot \ell+18\right) \ceil{\frac{M+1}{\ell}} +\ell\cdot (2\cdot M+7)+8\cdot (M+3) \right) \cdot \mathsf M,
\end{equation}
neglecting field additions and substractions, and
\item 
oracle costs of 
\begin{equation}
M + K+1 =  M + \ceil{\frac{M+1}{\ell}} + 1
\end{equation} 
functions of size $|H|$.
\end{itemize}

\subsubsection{The optimal choice of $\ell$}

As for Protocol \ref{prot:lookup}, the partial product size $\ell$ determines the trade-off between algebraic degree and the number of commitments.
Based on the multi-scalar multiplication benchmarks from Table \ref{tab:pippenger}, and the operation counts \eqref{e:plookup:prep:cost} and \eqref{e:plookup:sumcheck:cost} we obtain the following estimates for the optimal choice of $\ell$, see Table \ref{tab:plookup:optimal}.

\begin{table}[h!]
\caption{%
An estimate of the partial sum size $\ell$ which minimizes the prover cost. 
The numbers are based on the operation counts \eqref{e:plookup:prep:cost} and \eqref{e:plookup:sumcheck:cost} for the oracle prover and the benchmarks for a multi-scalar multiplication in the Pallas curve, Table \ref{tab:pippenger}.
}
\label{tab:plookup:optimal}
\vspace*{0.5cm}
\centering
\begin{tabular} {|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{$\log N$} & \multicolumn{4}{c|}{$\ell$} & \multicolumn{4}{c|}{field mult. per col.} 
\\\cline{2-9}
& $M=10$ & $M=20$ & $M=70$ & $M=\infty$ & $M=10$ &$M=20$ & $M=70$ & $M=\infty$
\\\hline
12 & 11 & 11 & 12 & 12 & $N\cdot 750$ & $N\cdot 717$ &  $N\cdot 687$ &  $N\cdot 675$
\\
14 & 11 & 11 & 12 & 11 & $N\cdot 637$ & $N\cdot 609$ &  $N\cdot 584$ &  $N\cdot 572$
\\
16 & 11 &11 & 9 & 10 & $N\cdot 553$ & $N\cdot 528$ &  $N\cdot 506$&  $N\cdot 496$
\\
18 & 11 & 11 & 9 &  10 & $N\cdot 503$ & $N\cdot 479$  &  $N\cdot 458$ &  $N\cdot 450$
\\\hline
\end{tabular}
\end{table}

%\newpage
\subsection{Bounded multiplicity encoding}
\label{s:bounded:plookup}

The Polygon MidenVM \cite{Miden} uses an improvement of Plookup,  which reduces the number of commitments for the sorted  union of $f_1,\ldots, f_M$ and $t$ by arranging it as runs of at most $B=2^b$ occurences, where $b\geq 1$.
The length $m$ of each run (reduced by one, as explained below) is then given bitwise,
\[
m - 1 = m_{0} + m_{1}\cdot 2 + \ldots + m_{b-1}\cdot 2^{b-1},
\]
and the power of the corresponding term $Z$ in the lookup product is selected by  the expression
\begin{equation}
\label{e:bplookup:V}
V(Z, m_0, \ldots, m_{b-1}) = \prod_{j=0}^{b-1} (m_{j}\cdot  Z^{2^j} + (1- m_{j})\cdot 1) = Z^{m_0 + m_1\cdot 2 + \ldots + m_{b-1}\cdot 2^{b-1}}.
\end{equation}
From this point of view it can be seen as an application of the Arya \cite{Arya} multiplicity technique to Plookup.
Although \cite{Miden} restricts to range proofs (optimized for the cases of small ranges, the size of which is a fraction of the domain), we quickly outline its generalization to lookups of domain-sized tables.

Let $N$ be the size of the punctuated hypercube $H'$. 
Instead of committing to the full-length  union $(s_i)_{i=1}^{(M+1)\cdot N}$, the prover provides the compressed sequence
\[
(r_j, ( m_{j,0}, \ldots,  m_{j,b-1}))_{j=1}^n
\]
of values $r_j$ and their $B$-bounded run lengths $m_j$ (minus one), represented bitwise by $ m_{j,0},\ldots,  m_{j,b-1} \in\{0,1\}$.  
Depending on the distribution of the values, the size $n$ of the compressed sequence is bounded by 
\[
\frac{M + 1}{B}\cdot N \leq n \leq \left(\frac{M}{B} + 1\right)\cdot N.
\]
(The left bound corrsponds to uniform distribution, and the right bound to the singleton case.)
The lookup product over $s$, written in terms of the compressed sequence is then
\begin{multline*}
\prod_{i=1}^{(M+1)\cdot N} (X + s_i+ s_{(i \bmod N) +1}\cdot Y)  
%= \prod_{j=1}^{n} (X +   (1 + Y)\cdot  s_j)^{ m_j - 1} \cdot (X +   s_j +  Y\cdot  s_{(j \bmod n) +1})
%\\
= \prod_{j=1}^{n} V(X + (1+Y)\cdot r_j, \underbrace{ m_0, \ldots,  m_{b-1}}_{\text{encoding }  m_j - 1}) \cdot (X +   r_j +  Y\cdot  r_{(j \bmod n) +1}).
\end{multline*}

As before, the compressed sequence is provided piecewise, splitted into functions over $H'$. 
This results in
\[
R = \ceil{\frac{M}{B}+ 1} 
\]
``blocks'' of functions, $r_k(\vec x)$ and $( m_{k,0}(\vec x),\ldots,  m_{k, b-1}(\vec x))$, $k=1,\ldots, R$, subject to 
the lookup identity
\begin{multline*}
%\label{e:bPlookup:lookup:identity}
%\begin{aligned}
\prod_{\vec x\in H'} 
V(X +  r_K(\vec x) \cdot (1 + Y), m_{R,0}(\vec x),\ldots, m_{R,b-1}(\vec x)) \cdot (X +  r_{R}(\vec x) +  r_{1}(T(\vec x))\cdot Y)
\\
\cdot 
\prod_{k=1}^{R-1}
V(X +  r_k(\vec x) \cdot (1 + Y), m_{k,0}(\vec x),\ldots, m_{k,b-1}(\vec x)) \cdot (X +  r_{k}(\vec x) +  r_{k+1}(\vec x)\cdot Y)
\\
= \prod_{\vec x\in H'} (X + t(\vec x) + t(T(\vec x))\cdot Y) \cdot \prod_{i=1}^M (X + f_i(\vec x)\cdot (1 + Y)),
%\end{aligned}
%\end{equation}
\end{multline*}
with $V$ as above, and the boolean domain identities $m_{k,i}(\vec x)\cdot (1 - m_{k,i}(\vec x))^2 = 0$,
over $H$, for all $k, i$.
Note that the commitment costs for $s$ are reduced by the factor 
\[
\frac{R\cdot (1 + \log_2 B)}{M+1} \approx \frac{1 + \log_2 B}{B},
\]
whereas the absolute degree of the lookup identity is almost invariant for large $B$, increasing only by the factor
\[
\frac{R}{M+1}\cdot (B + \log_2 B) \approx 1 + \frac{\log_2 B}{B}.
\]
As the protocol is along the lines of plookup, we omit further details.







%\newpage
\section{Comparison}


The significant advantage of the logarithmic derivative lookup over Plookup is the lower oracle costs.
While the Plookup strategy demands $M + 1$ oracles for committing to the sorted union of the $M$ witness column and the table, the logarithmic derivative approach demands only a single oracle for the multiplicities.
We compare the two strategies by their computational cost per column, when running them with optimal sum/product size $\ell$ as given in Table \ref{tab:lookup:optimal} and \ref{tab:plookup:optimal}.

\begin{table}[h!]
\caption{%
The estimated performance advantage of logarithmic derivative lookups (Protocol \ref{prot:lookup}) over the multivariate Plookup strategy (Section \ref{s:hyperplonk:small}), as the ratio of their number of field multiplications per column. 
The numbers are based on the equivalent number of field multiplications for a multi-scalar multiplication over the Pallas curve, given in Table \ref{tab:pippenger}.
}

\label{tab:comparison}
\vspace*{0.5cm}
\centering
\begin{tabular} {|c|c|c|c|c|c|c|}
\hline
\multirow{2}*{$\log|H|$} & \multicolumn{6}{c|}{ratio Plookup / log. derivative} 
\\
\cline{2-7}
& $M=1$ & $M=5$ & $M=10$ & $M=20$ & $M=70$ & $M=\infty$ 
\\\hline
12 & $1.5$ & $3.1$ &  $4.0$ & $4.5$ & $5.2$ & $5.6$
\\
14 & $1.5$ & $3.0$ & $3.8$  &$4.2$ & $4.8$ & $5.1$
\\
16 & $1.5$ & $2.9$ &$3.6$ & $4.0$ & $4.5$  & $4.8$
\\
18 & $1.5$ & $2.9$ &$3.4$ & $3.9$  & $4.3$ & $4.5$
\\\hline
\end{tabular}
\end{table}

Let us give a rough comparison with the bounded multiplicity improvement of Plookup described in Section \ref{s:bounded:plookup}.
For that  we simply count the number of oracles needed for obtaining a \textit{quadratic} overall domain identity (and hence a cubic sumcheck polynomial $Q$).
Under this assumption,

\begin{itemize}
\item
The logarithmic derivative approach needs overall $M + 1$ oracles: 
one for the multiplicity function, and $M$ helper functions.

\item
Plookup demands overall $2\cdot (M+1)$ oracles: 
$M+1$ for the sorted union of table and witnesses, and $M+1$ for the cumulative products.

\item
With multiplicity encoding using $b$ bits, Plookup is improved down to  
\[
M + 1 + \ceil{\frac{M} {2^b} + 1 }\cdot 3\cdot b
\] 
oracles:
Each of the $\ceil{\frac{M} {2^b} + 1}$ blocks of the compressed representation consists of $b + 1$ domain-sized functions (the multiplicity bits and the value), and to linearize each factor in the power selection expression \eqref{e:bplookup:V}, one additionally needs $b-1$ functions for the two-adic powers, and another $b$ functions for the products with the bits.
The grand product argument uses $M+1$ cumulative product functions.
\end{itemize}
%

In its most extreme setting, $2^b = M$, and therefore only $R=2$ blocks, the number of oracles for the multiplicity improvement is 
$M + 1 + 6 \cdot\log_2(M)$.
We use this configuration in our comparison, Table \ref{tab:comparison:bPlookup}. 
 
\begin{table}
\caption{%
A rough comparison of the three lookup protocols in their degree $d=2$ variant.
The table presents the overall number of commitments for a batch of $M=2^b$ columns. 
(bPlookup denotes Plookup using multiplicity encoding with $b$ bits.) 
}
\label{tab:comparison:bPlookup}
\centering
\vspace*{0.5cm}
\begin{tabular}{|c|c|c|c|c|c|c|}
\cline{2-7}
\multicolumn{1}{c|}{} &$M = 4$ & $M=8$ & $M=16$ & $M=32$ & $M=64$ & $M=128$
\\\hline
log. derivative & 5 & 9 & 17 & 33 & 65 & 129
\\
Plookup & 10 & 18 & 34 & 66 & 130 & 258
\\
bPlookup ($b=\log_2(M)$) & 17 & 27 & 41 & 63 & 101 & 171
\\\hline
\end{tabular}
\end{table}

Multiplicity encoding seems to be beneficial only for quite large batches of columns. 
Below $M = 32$ columns it is expected to perform even worse than Plookup. 
However, for $M=64$ and $M=128$ that picture clearly changes.  
For these batch sizes it consumes only $78\%$ and $66\%$ of Plookups' number of commitments, respectively, which is still about $55\%$ and  $30\%$ more than with the logarithmic derivative approach.

%To give an estimate for these costs in the case of an elliptic curve based Lagrange commitment scheme (such as the IPA), we 
%rely on a benchmark-backed equivalent of field multiplications for the multi-scalar multiplication in the Pallas curve.
%These are found in Table \ref{tab:pippenger}.
%With these equivalence measure and the operation counts from  \eqref{e:lookup:cost}, \eqref{e:lookup:large:cost} and \eqref{e:hyperplonk:cost}, \eqref{e:hyperplonk:large:cost}, we obtain the following ratios of field multiplications, see Table \ref{tab:comparison}.

%\begin{table}[h!]
%\caption{%
%The estimated performance advantage of logarithmic derivative lookups over the ones from this section, as the ratio of their number of field multiplications $r =\nicefrac{\mathsf M({Plookup})}{\mathsf M({logD})}$.
%The numbers are based on the equivalent number of field multiplications from Table \ref{tab:pippenger}.
%For hypercube sizes $|H|$ ranging from $2^{12}$--$2^{18}$ we describe the maximum ratio $r_{max}$ over the number of columns $M$, as well as the ranges for $M$ over which $r$ is larger than $2$ and $3$, respectively.
%The minimum ratio is throughout $r= 1.5$ and obtained in the single-column setting $M=1$. 
%}
%\label{tab:comparison}
%\vspace*{0.5cm}
%\centering
%\begin{tabular} {|c|c|c|c|c|}
%\hline
%$\log|H|$ & $r\geq 3$ & $r\geq 2$ & $r_{max}$ 
%\\\hline
%12 & $M\in [5, 41]$ & $M\in [3, 87]$ & $4.1$ (at $M =15$)
%\\
%14 & $M\in [6, 32]$ & $M\in [3, 71]$ & $3.8$ (at $M= 13$)
%\\
%16 & $M\in [6, 26]$  & $M\in [3, 59]$ & $3.5$ (at $M= 12$)
%\\
%18 & $M\in [6, 21]$  & $M\in [3, 52]$ & $3.2$ (at $M= 12$)
%\\\hline
%\end{tabular}
%\end{table}

\section{Acknowledgements}

The author would like to thank Rayan Matovu and Morgan Thomas for giving me the space and time to dwell on batch-column lookups. 
Special thanks to Marcin Bugaj for helping out with the Pippenger benchmarks.
Furthermore, I would like to thank Ariel Gabizon for his feedback and useful discussions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{alpha}
\bibliography{bibfileSNARKs}


\newpage
\appendix
\section{The flookup proof of radical}
\label{s:uv:flookup}

Section 5 of \cite{flookup} describes a polynomial IOP for lookups, which is almost identical to the logarithmic derivative approach. 
We present its generalization to batch-lookups. 
Let $F$ be an FFT-friendly finite field, having a multiplicative subgroup $H = \{x\in F: x^n = 1\}$ of order $n$, and denote by $g$ a generator of it.  
For showing that the ranges of witness function $f_i: H\rightarrow F$, $i=0, \ldots, M-1$, are contained in the range of a table $t: H\rightarrow F$, the fractional logaritmic derivative identity
%\[
%\sum_{x\in H} \frac{1}{X + f(x)} = \sum_{x\in H} \frac{m(x)}{X + t(x)}
%\]
is turned into the polynomial identity
\[
\sum_{x\in H} \sum_{i=0}^{M-1} \frac{v_T(X)}{X + f_i(x)} = \sum_{x\in H} m(x) \cdot \frac{v_T(X)}{X + t(x)},
\]
by multiplying with the precomputed table polynomial $v_T(X) = \prod_{x\in H} (X + t(x))$. 
Instead of the multiplicity function $m$, the prover explicitly provides the polynomial\footnote{%
In a variant of the protocol, communicated by A. Gabizon, the prover provides the Lagrange representation of $R_T(X)$ with respect to the Lagrange basis of the table set $\{t(x) : x\in H\}$. 
As Lagrange IOPs with respect to several Lagrange bases have different impacts on the Lagrange commitment scheme (e.g., it leads to a separate commitment in a KZG-like scheme, or a separate inner product argument for an IPA-like scheme), we do not compare with this variant.
}% 
\[
R_T(X) = \sum_{x\in H} m(x) \cdot \frac{v_T(X)}{X - t(x)},
\]
and engages with the verifier in an IOP for showing that
\begin{equation}
\sum_{x\in H}  \sum_{i=0}^{M-1} \frac{v_T(X)}{X + f_i(x)} = R_T(X).
\end{equation}
The verifier queries $v_T(X)$ and $R_T(X)$ at a random challenge $\alpha\sample F$, and both prover and verifier run a sumcheck argument for 
\[
\sum_{x\in H}  \sum_{i=0}^{M-1} \frac{1}{\varphi_i(x)} = \frac{R_T(\alpha)}{v_T(\alpha)},
\]
where $\varphi_i(x) = \alpha + f_i(x)$.
For this the prover provides the Lagrange representation of the sumcheck polynomial $\phi(X)$  subject to the domain identity
\[
\phi(g\cdot x) - \phi(x) + \frac{R_T(\alpha)}{|H|\cdot v_T(\alpha)} = \sum_{i=0}^{M-1} \frac{1}{\varphi_i(x)}
\]
for all $x\in H$, or 
\begin{equation}
\label{e:flookup:overall:identity}
\left(\phi(g\cdot X) - \phi(X) + \frac{R_T(\alpha)}{|H|\cdot v_T(\alpha)}\right) \cdot \prod_{i=0}^{M-1} \varphi_i(X) =  \prod_{i=0}^{M-1} \varphi_i(X)\cdot  \sum_{i=0}^{M-1} \frac{1}{\varphi_i(x)}  \mod v_H(X).
\end{equation}
The overall identity is of the form 
\[
Q(\phi_0(X), \ldots, \phi_{M-1}(X), \phi(X), \phi(g\cdot X)) = 0 \mod v_H(X),
\]
with $Q$ having $\nu = M + 2$ variables and absolute degree $d = M + 1$.
%The prover provides the Lagrange representation of the component polynomials $q_1(X), \ldots, q_M(X)$ for the overall quotient.
%The verifier samples $\beta\sample F$ and checks \eqref{e:flookup:overall:identity} at $X=\beta$ by quering the involved polynomials at the needed points. 



\end{document}
